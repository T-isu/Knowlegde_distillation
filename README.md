# Knowlegde_distillation
#
In this repo, we have included a file titled 'Untitled4' which contains several attempts at utilising a simple distillation loss with a faster r cnn and resnet 50 backbone teacher network and yolov5 srtudent network, after that we have also tried to use resnet101 teacher and resnet18 student but did not get any output.
#
The 'collate_fn.py' is used to handle the coco dataset.
#
The 'knowledge_dist-2.ipynb' file is given as a reference where we have used cosine loss and mse loss for a classification based knowledge distillation problem on the CIFAR-10 dataset.
